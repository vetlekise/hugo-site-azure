[{"content":"Introduction Every project starts simple, but when managing infrastructure for dev, test, and prod, let alone multiple customers, it quickly becomes complex. How can we solve this chaos?\nWe\u0026rsquo;ll tackle the common issues of scalable IaC, and go through a clear journey from basic patterns to something more advanced, showing you how to choose the right tool for the job.\nIsolated Folders This is the classic starting point when trying to figure out multi-environment setups. Its the simplest way to manage multiple environments by giving each one its own dedicated directory.\nPros Each environment has its own dedicated directory and state file, providing the highest level of safety and preventing accidental cross-environment changes. The structure is straightforward and very easy to understand, making it an excellent starting point for new projects or teams. The logic is intuitive, which is ideal for those new to Infrastructure as Code (IaC). Cons You must repeat boilerplate code (like providers and variables) for each environment, which violates the Don\u0026rsquo;t Repeat Yourself (DRY) principle. As the project grows, making a change to a shared component requires updating it in every single folder, which is tedious and error-prone. This pattern quickly becomes unmanageable and inefficient when dealing with a large number of environments. Project Structure Your repository will typically be organized into dev, test, and prod directories. Each environment then calls the modules from versioned repositories.\nNote: Instead of using separate repositories for your modules, you could create a modules directory. Problem with this is versioning and its difficult for others to reuse your module.\n1 2 3 4 5 6 7 8 9 10 application/ ├── dev/ │ ├── main.tf # Calls the versioned \u0026#39;core-infra\u0026#39; │ ├── variables.tf # Declares variables for the dev environment │ └── terraform.tfvars # Assigns values for dev │ └── prod/ ├── main.tf ├── variables.tf └── terraform.tfvars Configuration Examples Let\u0026rsquo;s say you have another repository acting as a Terraform module called core-infra and you want this deployed to the dev environment. This module has its own variables.tf with variable declarations (aka. input parameters). You then declare the required variables in your application repo dev/variables.tf:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 variable \u0026#34;vm_count\u0026#34; { description = \u0026#34;Number of Azure Virtual Machines.\u0026#34; type = number } variable \u0026#34;vm_size\u0026#34; { description = \u0026#34;The Azure VM size.\u0026#34; type = string } variable \u0026#34;location\u0026#34; { description = \u0026#34;The Azure region where resources will be deployed.\u0026#34; type = string } This is where the duplicate code comes in. You\u0026rsquo;ve now declared variables in dev/variables.tf, but now you want to deploy the same module to the prod environment, so now you have to duplicate the variables in prod/variables.tf\nTo provide values to the variables you add the following to dev/terraform.tfvars:\n1 2 3 vm_count = 1 vm_size = \u0026#34;Standard_B1s\u0026#34; location = \u0026#34;Norway East\u0026#34; In dev/main.tf is where you would call your reusable module core-infra.\n1 2 3 4 5 6 7 8 9 10 module \u0026#34;core_infra\u0026#34; { # Source points to a separate Git repo and a specific version tag source = \u0026#34;git@github.com:your-org/core-infra.git?ref=v1.0.0\u0026#34; # Pass values to the module environment = \u0026#34;dev\u0026#34; vm_count = var.vm_count vm_size = var.vm_size location = var.location } Why not hardcode all the values in the module call you ask?\nA .tfvars file acts as a clean, simple input sheet for an environment. Someone less familiar with Terraform would immediately see all the key parameters instead of needing to look in the configuration files. You can also easily override them using terraform apply -var=\u0026quot;location=westeurope\u0026quot; which makes automation easier.\nLocal Workflow To deploy the dev environment, navigate into its folder within the application repo and apply the configuration:\n1 2 3 4 5 6 7 8 9 10 # 1. Clone the live application repo and change into it git clone git@github.com:your-org/application.git cd application/dev # 2. Initialize Terraform (this will download the module from Git) terraform init # 3. Plan and Apply terraform plan terraform apply CI/CD Pipeline (GitHub Actions) The following workflow example is dynamic. It looks at the changed environments and builds a matrix to dynamically run plan and apply to the correct environment.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 name: \u0026#39;Terraform Folders\u0026#39; on: pull_request: types: [opened, synchronize, reopened] branches: [main] paths: - \u0026#39;dev/**\u0026#39; - \u0026#39;prod/**\u0026#39; push: branches: [main] paths: - \u0026#39;dev/**\u0026#39; - \u0026#39;prod/**\u0026#39; # Add permissions for writing PR comments. # You may need to add more permissions here for your cloud provider\u0026#39;s OIDC. permissions: pull-requests: write # Example for OIDC: # id-token: write # contents: read jobs: detect-changes: name: \u0026#39;Detect Changed Environments\u0026#39; runs-on: ubuntu-latest outputs: environments: ${{ steps.filter.outputs.all_changed_files }} steps: - name: \u0026#39;Checkout Code\u0026#39; uses: actions/checkout@v5 with: fetch-depth: 0 - name: \u0026#39;Find changed environment folders\u0026#39; id: filter uses: tj-actions/changed-files@v44 with: # Since dev/prod are in root, the base path is the repository root path: \u0026#39;.\u0026#39; dir_names: \u0026#34;true\u0026#34; json: true escape_json: false # Define the environment folders to watch for changes files: | dev/** prod/** - name: \u0026#39;Debug Output\u0026#39; run: | echo \u0026#34;Detected changes: ${{ steps.filter.outputs.all_changed_files }}\u0026#34; plan: name: \u0026#39;Plan for ${{ matrix.environment }}\u0026#39; runs-on: ubuntu-latest needs: detect-changes if: needs.detect-changes.outputs.environments != \u0026#39;[]\u0026#39; strategy: matrix: environment: ${{ fromJson(needs.detect-changes.outputs.environments) }} steps: - name: \u0026#39;Checkout Code\u0026#39; uses: actions/checkout@v5 - name: \u0026#39;Setup Terraform\u0026#39; uses: hashicorp/setup-terraform@v3 # Add your cloud provider login step here. - name: \u0026#39;Terraform Init\u0026#39; id: init working-directory: ${{ matrix.environment }} run: terraform init -no-color - name: \u0026#39;Terraform Plan\u0026#39; id: plan working-directory: ${{ matrix.environment }} run: terraform plan -no-color -out=tfplan - name: \u0026#39;Upload Plan Artifact\u0026#39; uses: actions/upload-artifact@v4 with: name: tfplan-${{ matrix.environment }} path: ${{ matrix.environment }}/tfplan - name: Post Plan Comment to PR if: steps.plan.outcome == \u0026#39;success\u0026#39; \u0026amp;\u0026amp; github.event_name == \u0026#39;pull_request\u0026#39; uses: actions/github-script@v8 env: PLAN: \u0026#34;${{ steps.plan.outputs.stdout }}\u0026#34; with: script: | const { PLAN } = process.env; const output = `#### Terraform Plan for \\`${{ matrix.environment }}\\` \u0026lt;details\u0026gt;\u0026lt;summary\u0026gt;Show Plan\u0026lt;/summary\u0026gt; \\`\\`\\`terraform ${PLAN} \\`\\`\\` \u0026lt;/details\u0026gt; *Pushed by: @${{ github.actor }}, Action: \\`${{ github.event_name }}\\`*`; await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: output }); apply: name: \u0026#39;Apply for ${{ matrix.environment }}\u0026#39; runs-on: ubuntu-latest needs: [detect-changes, plan] if: github.event_name == \u0026#39;push\u0026#39; \u0026amp;\u0026amp; github.ref == \u0026#39;refs/heads/main\u0026#39; \u0026amp;\u0026amp; needs.detect-changes.outputs.environments != \u0026#39;[]\u0026#39; strategy: matrix: environment: ${{ fromJson(needs.detect-changes.outputs.environments) }} steps: - name: \u0026#39;Checkout Code\u0026#39; uses: actions/checkout@v5 - name: \u0026#39;Setup Terraform\u0026#39; uses: hashicorp/setup-terraform@v3 # Add your cloud provider login step here (same as in the plan job). - name: \u0026#39;Download Plan Artifact\u0026#39; uses: actions/download-artifact@v5 with: name: tfplan-${{ matrix.environment }} path: ${{ matrix.environment }} - name: \u0026#39;Terraform Init\u0026#39; id: init working-directory: ${{ matrix.environment }} run: terraform init -no-color - name: \u0026#39;Terraform Apply\u0026#39; id: apply working-directory: ${{ matrix.environment }} run: terraform apply -auto-approve \u0026#34;tfplan\u0026#34; OpenTofu Workspaces Terraform has a feature called Workspaces that\u0026rsquo;s used to deploy the same code to multiple environments, keeping your configuration DRY. It still uses one backend but creates separate states for better isolation.\nWhile Terraform workspaces work, we are going to leverage an exclusive OpenTofu feature early variable evaluation to make our lives easier.\nFAILS in Terraform:\n1 2 3 4 5 6 # You cannot enable or disable a module based on the workspace. module \u0026#34;monitoring_alerts\u0026#34; { source = \u0026#34;./modules/monitoring\u0026#34; count = terraform.workspace == \u0026#34;prod\u0026#34; ? 1 : 0 # ERROR } WORKS in OpenTofu:\n1 2 3 4 5 6 # You can enable or disable a modules based on the workspace. module \u0026#34;monitoring_alerts\u0026#34; { source = \u0026#34;./modules/monitoring\u0026#34; count = tofu.workspace == \u0026#34;prod\u0026#34; ? 1 : 0 } Pros It keeps your codebase clean by using a single set of configuration files for all environments, eliminating boilerplate code. Workspaces provide a safe way to manage separate state files for each environment while using the same backend configuration. You can efficiently manage a single application or service across multiple similar environments from one place. Cons As environments diverge, the code can become cluttered with complex conditional logic (count, for_each), making it hard to read and maintain. A mistake in the single codebase can potentially affect all environments, as they are not fully isolated at the code level. The pattern is less suitable for managing vastly different environments, as forcing all variations into one set of files leads to overly complicated configurations. Project Structure With workspaces, your directory structure becomes quite simple. All your config for the application lives in a single folder:\n1 2 3 4 5 6 application/ ├── main.tf # The main logic. ├── variables.tf # A SINGLE declaration of all variables ├── dev.tfvars # Values for the \u0026#39;dev\u0026#39; environment ├── prod.tfvars # Values for the \u0026#39;prod\u0026#39; environment └── backend.tf # Defines the remote state backend Note: Example above only uses a main.tf file, but there\u0026rsquo;s nothing stopping you from creating more configurations!\nConfiguration Examples variables.tf\nVariables are declared only once.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 variable \u0026#34;vm_count\u0026#34; { description = \u0026#34;The number of Azure Virtual Machines.\u0026#34; type = number } variable \u0026#34;vm_size\u0026#34; { description = \u0026#34;The Azure VM size (e.g., \u0026#39;Standard_B1s\u0026#39;).\u0026#34; type = string } variable \u0026#34;location\u0026#34; { description = \u0026#34;The Azure region where resources will be deployed.\u0026#34; type = string } \u0026lt;insert_env\u0026gt;.tfvars\nProvides specific values for each environment.\n1 2 3 vm_count = 1 vm_size = \u0026#34;Standard_B1s\u0026#34; location = \u0026#34;Norway East\u0026#34; main.tf\nHandle resource creation with conditionals.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # This main module is enabled for all workspaces module \u0026#34;core_infra\u0026#34; { source = \u0026#34;git@github.com:your-org/core-infra.git?ref=v1.0.0\u0026#34; vm_count = var.vm_count vm_size = var.vm_size location = var.location environment = tofu.workspace # The workspace name is used to tag resources } # The monitoring module is conditionally enabled only in the \u0026#39;prod\u0026#39; environment module \u0026#34;monitoring_alerts\u0026#34; { source = \u0026#34;git@github.com:your-org/monitoring-alerts.git?ref=v1.2.0\u0026#34; # This works perfectly in OpenTofu, allowing dynamic environments count = tofu.workspace == \u0026#34;prod\u0026#34; ? 1 : 0 } Local Workflow The workflow involves selecting the correct workspace context before applying.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 1. Clone the application repo and change into it git clone git@github.com:your-org/application.git cd application # 2. Initialize OpenTofu (this will download the module from Git) tofu init # 3. Create workspaces tofu workspace new dev tofu workspace new prod # 4. Deploy to \u0026#39;dev\u0026#39; # 4.1 Switch to the \u0026#39;dev\u0026#39; workspace tofu workspace select dev # 4.2 Plan and apply, specifying the correct .tfvars file tofu plan -var-file=\u0026#34;dev.tfvars\u0026#34; tofu apply -var-file=\u0026#34;dev.tfvars\u0026#34; CI/CD Pipeline (GitHub Actions) This pipeline uses a GitHub Actions feature called reusable workflows to keep your pipeline DRY and easy to manage. The logic is split into two files: a reusable worker that performs the deployment, and a main orchestrator that defines the release process.\nreusable-worker.yml\nContains all the steps to deploy to any single environment. It accepts an environment name as an input, which it uses to dynamically select the correct OpenTofu workspace and tfvars file. This means you only have to define your deployment logic once.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 name: \u0026#39;Tofu Reusable Worker\u0026#39; on: workflow_call: inputs: environment: required: true type: string plan_only: required: false type: boolean default: false permissions: id-token: write contents: read pull-requests: write jobs: tofu: name: \u0026#34;Tofu ${{ inputs.plan_only \u0026amp;\u0026amp; \u0026#39;Plan\u0026#39; || \u0026#39;Apply\u0026#39; }} on ${{ inputs.environment }}\u0026#34; runs-on: ubuntu-latest environment: ${{ inputs.environment }} steps: - name: \u0026#39;Checkout Code\u0026#39; uses: actions/checkout@v4 - name: \u0026#39;Setup OpenTofu\u0026#39; uses: opentofu/setup-opentofu@v1 # Add your cloud provider login here. - name: \u0026#39;Tofu Init\u0026#39; run: tofu init - name: \u0026#39;Select or Create Workspace\u0026#39; run: tofu workspace select -or-create ${{ inputs.environment }} - name: \u0026#39;Tofu Validate\u0026#39; run: tofu validate -no-color - name: \u0026#39;Tofu Plan\u0026#39; id: plan run: tofu plan -var-file=\u0026#34;${{ inputs.environment }}.tfvars\u0026#34; -no-color -out=tfplan continue-on-error: ${{ inputs.plan_only }} - name: \u0026#39;Post Plan Comment to PR\u0026#39; if: inputs.plan_only \u0026amp;\u0026amp; github.event_name == \u0026#39;pull_request\u0026#39; uses: actions/github-script@v7 env: PLAN: \u0026#34;tofu\\n${{ steps.plan.outputs.stdout }}\u0026#34; with: script: | const { PLAN } = process.env; const output = `#### OpenTofu Plan 📖 \\`${{ github.event.pull_request.head.sha }}\\` for \\`${{ inputs.environment }}\\` \u0026lt;details\u0026gt;\u0026lt;summary\u0026gt;Show Plan\u0026lt;/summary\u0026gt; \\`\\`\\`\\n${PLAN}\\n\\`\\`\\` \u0026lt;/details\u0026gt; *Pushed by: @${{ github.actor }}, Action: \\`${{ github.event_name }}\\`*`; await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: output }); if (\u0026#34;${{ steps.plan.outcome }}\u0026#34; == \u0026#34;failure\u0026#34;) { process.exit(1); } - name: \u0026#39;Tofu Apply\u0026#39; if: inputs.plan_only == false \u0026amp;\u0026amp; steps.plan.outcome == \u0026#39;success\u0026#39; run: tofu apply -auto-approve \u0026#34;tfplan\u0026#34; deploy-orchestrator.yml\nThe main pipeline orchestrates the release by calling the reusable workflow for each stage. The needs: keyword creates a promotion chain, ensuring dev deploys first, followed by test, and finally prod. Each job simply passes the correct environment name to the reusable workflow.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 name: \u0026#39;Tofu Deploy Orchestrator\u0026#39; on: push: branches: - main pull_request: branches: - main permissions: id-token: write contents: read pull-requests: write jobs: plan: name: \u0026#39;Plan for PR\u0026#39; if: github.event_name == \u0026#39;pull_request\u0026#39; uses: ./.github/workflows/opentofu_workspaces_reusable.yml with: environment: dev plan_only: true deploy-dev: name: \u0026#39;Deploy to DEV\u0026#39; if: github.event_name == \u0026#39;push\u0026#39; uses: ./.github/workflows/opentofu_workspaces_reusable.yml with: environment: dev deploy-test: name: \u0026#39;Promote to TEST\u0026#39; if: github.event_name == \u0026#39;push\u0026#39; needs: deploy-dev uses: ./.github/workflows/opentofu_workspaces_reusable.yml with: environment: test deploy-prod: name: \u0026#39;Promote to PROD\u0026#39; if: github.event_name == \u0026#39;push\u0026#39; needs: deploy-test uses: ./.github/workflows/opentofu_workspaces_reusable.yml with: environment: prod Terragrunt Explicit Stacks As your application grows, its infrastructure often evolves from a single component into a stack of several interdependent services. Managing the deployment order and dependencies of a complex stack with plain OpenTofu/Terraform can become cumbersome.\nThis is where Terragrunt, a thin wrapper for OpenTofu and Terraform, becomes essential. It excels at managing multi-component applications and keeping your configurations DRY. Specifically, the modern Explicit Stack pattern provides a powerful \u0026ldquo;blueprint\u0026rdquo; model to define and generate your entire infrastructure.\nPros Define an entire stack once in a terragrunt.stack.hcl blueprint, then easily create copies for dev, staging, and prod. See the complete composition of an environment in a single, clear blueprint file. Terragrunt builds a deployment graph from your blueprint, automatically ensuring the correct deployment order. Plan or apply an entire environment with a single command, like terragrunt stack run apply. Cons Requires learning the advanced blueprint concepts of unit blocks, values, and the generation process. Auto-generated files in the .terragrunt-stack directory can make debugging feel less direct than with simpler models. The required structure (units, stacks) can be overkill for smaller projects. Project Structure Terragrunt separates your live infrastructure configuration from your reusable modules. Your Terraform modules live in their own versioned Git repositories, while your live infrastructure repository contains the units (wrappers) and stacks (blueprints).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 application/ ├── root.hcl # Root config (backend, common variables) │ ├── units/ │ ├── resource_group/ │ │ └── terragrunt.hcl # Reusable wrapper for a resource_group module │ └── storage_account/ │ └── terragrunt.hcl # Reusable wrapper for a storage_account module │ └── stacks/ └── dev/ │ └── terragrunt.stack.hcl # THE BLUEPRINT for the \u0026#39;dev\u0026#39; environment └── prod/ └── terragrunt.stack.hcl # THE BLUEPRINT for the \u0026#39;prod\u0026#39; environment Configuration Examples root.hcl\nThis file, at the top of your repository, defines configurations that are inherited by all other modules, eliminating repetition.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # Configure the remote state backend ONCE for all modules. remote_state { backend = \u0026#34;azurerm\u0026#34; generate = { path = \u0026#34;backend.tf\u0026#34; if_exists = \u0026#34;overwrite_terragrunt\u0026#34; } config = { use_azuread_auth = true use_oidc = true resource_group_name = \u0026#34;\u0026#34; storage_account_name = \u0026#34;\u0026#34; container_name = \u0026#34;\u0026#34; key = \u0026#34;${path_relative_to_include()}/terraform.tfstate\u0026#34; # e.g. dev/vnet/terraform.tfstate } } # Generate an Azure provider block for every module generate \u0026#34;provider\u0026#34; { path = \u0026#34;provider.tf\u0026#34; if_exists = \u0026#34;overwrite_terragrunt\u0026#34; contents = \u0026lt;\u0026lt;EOF provider \u0026#34;azurerm\u0026#34; { features {} } EOF } # Define common inputs for all modules in this repo inputs = { location = \u0026#34;norwayeast\u0026#34; } units/resource_group/terragrunt.hcl\nThis is a reusable \u0026ldquo;unit template\u0026rdquo; that wraps your Terraform module. The values object is used to access variables passed down from the blueprint.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 include \u0026#34;root\u0026#34; { path = find_in_parent_folders(\u0026#34;root.hcl\u0026#34;) } terraform { # The source URL is in from the blueprint source = values.module_source } # The inputs are also passed in from the blueprint\u0026#39;s \u0026#39;values\u0026#39; inputs = { name = values.name environment = values.environment } units/storage_account/terragrunt.hcl\nThis unit template shows how to define a dependency. The config_path uses a variable that will be provided by the blueprint.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 include \u0026#34;root\u0026#34; { path = find_in_parent_folders(\u0026#34;root.hcl\u0026#34;) } terraform { # The source URL is passed in from the blueprint source = values.module_source } dependency \u0026#34;resource_group\u0026#34; { # The path to the dependency is passed in from the blueprint config_path = values.resource_group_path mock_outputs = { # The mock value is passed in from the blueprint rg_name = values.mock_rg_name } } inputs = { name = values.name environment = values.environment rg_name = dependency.resource_group.outputs.rg_name } stacks/dev/terragrunt.stack.hcl\nThis is the blueprint. It assembles the final environment by pointing to the reusable units and providing the specific values for dev.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 unit \u0026#34;resource_group\u0026#34; { # Source points to our reusable Unit Template source = \u0026#34;../../units/resource_group\u0026#34; path = \u0026#34;resource_group\u0026#34; # These \u0026#39;values\u0026#39; are passed as variables to the unit\u0026#39;s terragrunt.hcl values = { # This points to your versioned Terraform module repository module_source = \u0026#34;git::git@github.com:your-org/terraform-azurerm-resource-group.git?ref=v1.0.0\u0026#34; # Module-specific inputs name = \u0026#34;grunt-dev\u0026#34; environment = \u0026#34;dev\u0026#34; } } unit \u0026#34;storage_account\u0026#34; { source = \u0026#34;../../units/storage_account\u0026#34; path = \u0026#34;storage_account\u0026#34; values = { # This points to your versioned Terraform module repository module_source = \u0026#34;git::git@github.com:your-org/terraform-azurerm-storage-account.git?ref=v1.2.0\u0026#34; # Module-specific inputs name = \u0026#34;gruntdev932847\u0026#34; environment = \u0026#34;dev\u0026#34; # We pass the relative path that the dependency # block in the unit template needs to find the other generated unit. resource_group_path = \u0026#34;../resource_group\u0026#34; # We also pass the mock value to keep the unit template generic. mock_rg_name = \u0026#34;grunt-dev\u0026#34; } } Local Workflow To deploy the entire dev stack, you run a stack command from the directory containing the blueprint.\n1 2 3 4 5 6 7 8 # Clone repository git clone git@github.com:your-org/application.git # Navigate to the blueprint\u0026#39;s folder cd stacks/dev # This single command generates, plans, and applies the entire stack in the correct order. terragrunt stack run plan terragrunt stack run apply CI/CD Pipeline (GitHub Actions) Similar to how we did the OpenTofu pipeline, this uses a reusable workflow to create a promotion path from dev to prod. The entire process is driven by the stack blueprints.\nreusable-worker.yml\nThis workflow performs the plan or apply for a single environment. It\u0026rsquo;s cloud-agnostic and runs from the repository root.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 name: \u0026#39;Terragrunt Reusable Worker\u0026#39; on: workflow_call: inputs: environment: required: true type: string # e.g., \u0026#34;dev\u0026#34;, \u0026#34;prod\u0026#34; plan_only: required: false type: boolean default: false permissions: id-token: write # For cloud OIDC login contents: read jobs: terragrunt: name: \u0026#34;Terragrunt ${{ inputs.plan_only \u0026amp;\u0026amp; \u0026#39;Plan\u0026#39; || \u0026#39;Apply\u0026#39; }} on ${{ inputs.environment }}\u0026#34; runs-on: ubuntu-latest environment: ${{ inputs.environment }} steps: - name: \u0026#39;Checkout Code\u0026#39; uses: actions/checkout@v4 - name: \u0026#39;Setup OpenTofu \u0026amp; Terragrunt\u0026#39; uses: gruntwork-io/terragrunt-action@v3 with: tg_version: \u0026#39;v0.90.0\u0026#39; tofu_version: \u0026#39;1.10.6\u0026#39; # Add your cloud provider login - name: \u0026#39;Terragrunt Plan\u0026#39; if: inputs.plan_only working-directory: stacks/${{ inputs.environment }} run: terragrunt stack run plan - name: \u0026#39;Terragrunt Apply\u0026#39; if: inputs.plan_only == false working-directory: stacks/${{ inputs.environment }} run: terragrunt stack run apply --non-interactive deploy-orchestrator.yml\nThis main pipeline orchestrates the release process by calling the reusable worker for each environment.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 name: \u0026#39;Terragrunt Deploy Orchestrator\u0026#39; on: push: branches: [main] pull_request: branches: [main] paths: - \u0026#39;**.hcl\u0026#39; - \u0026#39;.github/workflows/**\u0026#39; jobs: plan-dev: name: \u0026#39;Plan Dev for PR\u0026#39; if: github.event_name == \u0026#39;pull_request\u0026#39; uses: ./.github/workflows/reusable-worker.yml with: environment: dev plan_only: true secrets: inherit deploy-dev: name: \u0026#39;Deploy to DEV\u0026#39; if: github.ref == \u0026#39;refs/heads/main\u0026#39; \u0026amp;\u0026amp; github.event_name == \u0026#39;push\u0026#39; uses: ./.github/workflows/reusable-worker.yml with: environment: dev secrets: inherit deploy-prod: name: \u0026#39;Promote to PROD\u0026#39; if: github.ref == \u0026#39;refs/heads/main\u0026#39; \u0026amp;\u0026amp; github.event_name == \u0026#39;push\u0026#39; needs: deploy-dev uses: ./.github/workflows/reusable-worker.yml with: environment: prod secrets: inherit Conclusion There is no single best solution, only the right one for your project\u0026rsquo;s current scale and complexity.\nFeature Isolated Folders OpenTofu Workspaces Terragrunt Explicit Stacks Simple to Start ✅ ✅ 🟡 DRY (No Repetition) ❌ ✅ ✅ Strong Code Isolation ✅ 🟡 ✅ Dependency Management ❌ ❌ ✅ Advanced Blueprints ❌ ❌ ✅ Scales for Complex Stacks ❌ 🟡 ✅ Note: The GitHub Actions workflows in this article are robust foundations. I encourage you to adapt the triggers, promotion rules, and cloud provider steps to fit your team\u0026rsquo;s specific needs and policies.\nResources \u0026amp; Links For more detailed information on stuff used in this article, please refer to the official documentation.\nOpenTofu Workspaces Terragrunt Explicit Stacks Configuration Examples GitHub Actions ","date":"2025-10-10T00:00:00Z","permalink":"http://localhost:1313/post/multi-environment-terraform/","title":"Manage Multiple Terraform Environments"},{"content":"What is Terraform Drift? Drift is the term for when the real-world state of your infrastructure differs from the state defined in your configuration.\n― Christie Koehler, HashiCorp Blog It can be caused by many things, but urgent hotfixes and teams being unfamiliar with Infrastructure as Code (IaC) practices is the most likely cause. Its a significant problem because it undermines the core benefits of IaC.\nThe main issue is the loss of a single source of truth. When your code and infra tell different stories, you can no longer trust your code to be an accurate representation of your environment.\nApplying GitOps Principals to Infrastructure GitOps is an operational framework that takes DevOps best practices used for application development such as version control, collaboration, compliance, and CI/CD, and applies them to infrastructure automation.\n― GitLab Topics The core idea is simple:\nDescribe your entire desired infrastructure in a Git repository using declarative code. Automate the process that coninuously compares this desired state with the actual state of your live infrastructure. Correct any detected differences, ensuring the live environment always reflects the state defined in Git. By adopting this workflow, you treat your infrastructure the same as your application code. Changes are made via pull requests, reviewed by peers, and automatically deployed. This creates an audit trail and, most importantly, provides a mechanism for automatically correcting drift.\nDesigning the Reconciliation Pipeline To combat drift, we can build an automated reconciliation pipeline. This pipeline will periodically check for discrepancies and take action. Here\u0026rsquo;s two approaches:\nDetection-only The pipeline runs terraform plan on a schedule. If it detects any differences, it doesn\u0026rsquo;t apply them. Instead, it sends an alert to your team via Slack, creates a GitHub issue, or logs a warning. This approach is safer and gives your team full control over when and how to resolve the drift. Auto-correction This is the full GitOps approach. The pipeline runs terraform plan to detect drift and, if any is found, immediately runs terraform apply to automatically revert the infrastructure to the state defined in your code. This ensures your infrastructure is always in sync, but this requires a high degree of confidence in your automation and testing. Building the Pipeline with GitHub Actions Our goal is to create the auto-correction pipeline that runs on a schedule, checks for drift, and automatically applies the correct config if any drift is found.\nCreate a new file in your repository at .github/workflows/terraform-reconcile.yml and add the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 name: \u0026#39;Terraform Drift Reconciliation\u0026#39; on: # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # Runs on a schedule (e.g., every day at 2:00 AM UTC) schedule: - cron: \u0026#39;0 2 * * *\u0026#39; jobs: terraform-reconcile: name: \u0026#39;Terraform Reconcile\u0026#39; runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v5 - name: Setup Terraform uses: hashicorp/setup-terraform@v3 # ... Add step for authenticating to provider - name: Terraform Init id: init run: terraform init - name: Terraform Plan continue-on-error: true id: plan run: | plan_output=$(terraform plan -no-color -out=tfplan 2\u0026gt;\u0026amp;1) echo \u0026#34;${plan_output}\u0026#34; echo \u0026#34;drift_detected=false\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;plan_text\u0026lt;\u0026lt;EOF\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;${plan_output}\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;EOF\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT if echo \u0026#34;${plan_output}\u0026#34; | grep -q \u0026#39;Plan:\u0026#39;; then if ! echo \u0026#34;${plan_output}\u0026#34; | grep -q \u0026#39;No changes.\u0026#39;; then echo \u0026#34;Drift detected. Setting drift_detected=true\u0026#34; echo \u0026#34;drift_detected=true\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT fi fi if ! echo \u0026#34;${plan_output}\u0026#34; | grep -q -e \u0026#39;Plan:\u0026#39; -e \u0026#39;No changes.\u0026#39;; then echo \u0026#34;::error::Terraform plan failed with an error.\u0026#34; exit 1 fi - name: Terraform Apply if: steps.plan.outputs.drift_detected == \u0026#39;true\u0026#39; run: terraform apply -auto-approve \u0026#34;tfplan\u0026#34; - name: Post Summary - Drift Found if: steps.plan.outputs.drift_detected == \u0026#39;true\u0026#39; env: PLAN_TEXT: ${{ steps.plan.outputs.plan_text }} run: | echo \u0026#34;## Terraform Drift Reconciliation Report\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY echo \u0026#34;Drift was detected and automatically corrected at $(date).\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY echo \u0026#34;\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY echo \u0026#34;\u0026lt;details\u0026gt;\u0026lt;summary\u0026gt;View Terraform Plan\u0026lt;/summary\u0026gt;\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY echo \u0026#34;\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY echo \u0026#39;```terraform\u0026#39; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY echo \u0026#34;$PLAN_TEXT\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY echo \u0026#39;```\u0026#39; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY echo \u0026#34;\u0026lt;/details\u0026gt;\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY - name: Post Summary - Drift Not Found if: steps.plan.outputs.drift_detected == \u0026#39;false\u0026#39; run: | echo \u0026#34;## Terraform Drift Reconciliation Report\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY echo \u0026#34;No infrastructure drift was detected at $(date).\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY - name: Post Summary - Plan Error if: steps.plan.outcome == \u0026#39;failure\u0026#39; run: | echo \u0026#34;## Terraform Drift Reconciliation Report\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY echo \u0026#34;The Terraform plan step failed with an error. Please check the logs for details.\u0026#34; \u0026gt;\u0026gt; $GITHUB_STEP_SUMMARY This workflow builds a custom output (steps.plan.outputs.drift_detected) during the plan step that is used by the other steps.\nI tried using the -detailed-exitcode flag for terraform plan in combination with conditionals for posting summaries, but GitHub Actions must have some sort of bug, because it didn\u0026rsquo;t output the correct exit codes.\nBest Practices and Considerations Before deploying this in a production environment, consider the following:\nStart with detection-only Begin by removing the terraform apply step and adding a notification step instead. Let the pipeline run for a while to see how often drift occurs. Limit scope Initially, run this pipeline only on non-critical environments, like development. Secure your credentials OIDC is the most secure way to authenticate with your provider, as it provides short-lived, automatically rotating credentials. Use notifications Even with auto-correction, you can still notify your team when drift is detected and corrected. Consider adding a step to send a message to a Slack channel to keep everyone informed. Role-based access control (RBAC) The user that did a manual change (ClickOps), do they really need permission to do so? Conclusion Drift is natural when managing complex systems, expecially in immature environments where processes haven\u0026rsquo;t been fully developed. By implementing an automated reconciliation pipeline, you eliminate configuration drift, and create a more stable and predictable environment.\n","date":"2025-10-09T00:00:00Z","permalink":"http://localhost:1313/post/solve-terraform-drift/","title":"Solve Terraform Drift"},{"content":"Test dsadsa Note Useful information that users should know.\nTip Helpful advice for doing things better or more easily.\nImportant Key information users need to know to achieve their goal.\nWarning Urgent info that needs immediate user attention to avoid problems.\nCaution Advises about risks or negative outcomes of certain actions.\n","date":"2025-10-09T00:00:00Z","permalink":"http://localhost:1313/post/test/","title":"Test"},{"content":"Monorepo vs Multi-repo A Terraform Monorepo (or Terralith), keeps all of your Terraform modules in a single Git repository, often in a modules directory. While simple at first, this pattern makes it difficult for other teams to reuse your modules and complicates versioning. It\u0026rsquo;s generally considered an anti-pattern unless the modules are highly specific to a single application.\nInstead, you should almost always use the multi-repo pattern. Each module lives in its own dedicated Git repository. This approach makes your module a standalone component that is easy to share, maintain, and version. Consumers of your module can then reference a specific version using a Git tag.\n1 2 3 4 5 6 module \u0026#34;example\u0026#34; { source = \u0026#34;github.com/organization/repository?ref=v1.0.0\u0026#34; # Use commit hash to prevent supply chain attacks. # source = \u0026#34;github.com/organization/repository-name?ref=a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6q7r8s9t0\u0026#34; } Writing A well-written module is like a function, it accepts inputs, performs an action and returns outputs.\nVariables as an API Treat your variables as the public API of your module. Every variable should have a clear description, type, and a default value if possible. This make it easier to use and understand. Focused outputs Use outputs to expose important resource attributes that consumers of the module might need. Keep it simple A good module does one thing well. For example, create one module for networking and another for a database. This makes your modules more flexible. Example project structure:\n1 2 3 4 5 6 7 8 9 ├── README.md # Documentation and quick usage ├── main.tf # General configuration ├── variables.tf ├── outputs.tf ├── versions.tf # Required providers └── examples/ # More module usage examples └── advanced/ ├── main.tf └── outputs.tf Documentation Good documentation is important for making your modules usable. Manually writing it tho, is boring as heck. Instead, you should automatically generate it from your code using a tool like terraform-docs.\nIt scans your .tf files and updates your README.md file with required versions, descriptions, types, and default values for all of your variables and outputs.\nTo use it, run the following command in your module\u0026rsquo;s root directory:\n1 2 # This command generates a markdown table and updates your README. terraform-docs markdown table --output-file README.md . You should ideally run this in a pre-commit hook or a CI/CD pipeline to ensure your documentation is always up-to-date with your code.\nVersioning Versioning allows users to consume your module without worrying about unexpected breaking changes. The standard practice is to use Semantic Versioning (SemVer), which follows a MAJOR.MINOR.PATCH format.\nMAJOR (1.0.0 \u0026gt; 2.0.0): For incompatible or breaking changes. MINOR (1.0.0 \u0026gt; 1.1.0): For adding new features in a backward-compatible way. PATCH (1.0.0 \u0026gt; 1.0.1): For backward-compatible bug fixes. To release a new version in Git, you simply create and push a new Git tag.\n1 2 3 4 5 # Create a new tag for your first major release git tag v1.0.0 # Push the tag to your remote repository git push origin v1.0.0 This process can also be automated with a tool like release-drafter. This can be easily implemented in a GitHub Actions pipeline. It builds a release draft with a changelog based on labels in pull requests. You configure everything in a .github/release-drafter.yml file.\nUsing a Repository Template To save you the trouble of creating a module template yourself, I\u0026rsquo;ve created my own template you\u0026rsquo;re free to use - vetlekise/terraform-module-template\nThis template includes pre-commit hooks for linting, compliance scanning, and vulnerability scanning. This catches most issues before they\u0026rsquo;re even pushed.\nIt also includes a CI/CD pipeline using GitHub Actions for linting, documentation, and releases. This catches stuff that gets past the pre-commit hooks. It also automatically builds your documentation, and automatically creates a release draft with a changelog based on labels in pull requests.\nConclusion By following these practices, you can create Terraform modules that are robust, reusable, and easy to maintain. Here\u0026rsquo;s a short summary of the most important points:\nUse multi-repo project structure if you want the module to be easily maintainable and accessible to others. Look at variables as input parameters for the user. Version your modules using Semantic Versioning (X.Y.Z) Automate your documentation and versioning Resources \u0026amp; Links pre-commit terraform hooks terraform-docs release-drafter ","date":"2025-10-08T00:00:00Z","permalink":"http://localhost:1313/post/develop-terraform-modules/","title":"Develop Terraform Modules"},{"content":"What is OIDC? OIDC stands for OpenID Connect. It\u0026rsquo;s an identity layer built on top of the OAuth 2.0 framework.\nIn simple terms, it allows one application (like a GitHub Actions workflow) to securely prove its identity to another application (like a cloud provider such as Azure) without using static, long-lived secrets. Instead, the system uses a short-lived, verifiable ID Token. This process of exchanging a temporary token is often called a \u0026ldquo;federated identity\u0026rdquo; workflow and is a much more secure way to manage authentication for automated processes.\nPrerequisites Before you begin, ensure you have the following.\nA GitHub repository must already be created. An Microsoft account with an active Azure Subscription. Make sure the account has the Entra ID role Application Administrator, and Azure Owner role assigned to the subscription. In addition, the subscription needs the Microsoft.Storage resource provider registered. Locally installed tools: Git, Azure CLI, and a code editor. Azure Resources First, we need to create a few Azure resources, specifically a Resource Group containing a Storage Account. This will store our Terraform state file securely and offer state locking. The below Azure CLI script will create these resources for us, you just need to provide some values first by exporting them as environment variables.\n1 2 3 4 export RESOURCE_GROUP_NAME=\u0026#34;terraform\u0026#34; export STORAGE_ACCOUNT_NAME=\u0026#34;tfstate\u0026lt;INSERT RANDOM NUMBERS\u0026gt;\u0026#34; export CONTAINER_NAME=\u0026#34;tfstate\u0026#34; export LOCATION=\u0026#34;\u0026#34; Now run the below script. When you\u0026rsquo;re prompted for login, use the Azure account that has permissions to create resources in your desired Subscription.\n1 2 3 4 5 6 7 az login az group create --name $RESOURCE_GROUP_NAME --location $LOCATION az storage account create --name $STORAGE_ACCOUNT_NAME --resource-group $RESOURCE_GROUP_NAME --sku Standard_LRS --encryption-services blob az storage container create --name $CONTAINER_NAME --account-name $STORAGE_ACCOUNT_NAME You should now have something like this in your Subscription: Service Principal For the pipeline to authenticate, we need to create a Service Principal in Entra ID. The below script creates this for us, in addition to that, it creates the federated tokens needed for GitHub Actions to authenticate to Azure, and it also creates a couple of RBAC assignments needed for Terraform to manage the state file and create resources. Before running, export these environment variables:\n1 2 3 export APP_NAME=\u0026#34;GitHub Actions - OIDC\u0026#34; export YOUR_GITHUB_ORG=\u0026#34;\u0026#34; export YOUR_REPO_NAME=\u0026#34;\u0026#34; Now run the below script. You should already be logged in from the previous script, if not, add az login.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 AZURE_SUBSCRIPTION_ID=$(az account show --query id -o tsv) AZURE_CLIENT_ID=$(az ad app create --display-name \u0026#34;$APP_NAME\u0026#34; --query appId -o tsv) az ad sp create --id $AZURE_CLIENT_ID az role assignment create \\ --role \u0026#34;Contributor\u0026#34; \\ --assignee-object-id $(az ad sp show --id $AZURE_CLIENT_ID --query id -o tsv) \\ --assignee-principal-type ServicePrincipal \\ --scope \u0026#34;/subscriptions/$AZURE_SUBSCRIPTION_ID\u0026#34; az role assignment create \\ --role \u0026#34;Storage Blob Data Contributor\u0026#34; \\ --assignee-object-id $(az ad sp show --id $AZURE_CLIENT_ID --query id -o tsv) \\ --assignee-principal-type ServicePrincipal \\ --scope \u0026#34;/subscriptions/$AZURE_SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP_NAME/providers/Microsoft.Storage/storageAccounts/$STORAGE_ACCOUNT_NAME\u0026#34; az ad app federated-credential create \\ --id $AZURE_CLIENT_ID \\ --parameters \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;github-main-branch\u0026#34;,\u0026#34;issuer\u0026#34;:\u0026#34;https://token.actions.githubusercontent.com\u0026#34;,\u0026#34;subject\u0026#34;:\u0026#34;repo:\u0026#39;$YOUR_GITHUB_ORG\u0026#39;/\u0026#39;$YOUR_REPO_NAME\u0026#39;:ref:refs/heads/main\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Trust main branch\u0026#34;,\u0026#34;audiences\u0026#34;:[\u0026#34;api://AzureADTokenExchange\u0026#34;]}\u0026#39; az ad app federated-credential create \\ --id $AZURE_CLIENT_ID \\ --parameters \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;github-pull-requests\u0026#34;,\u0026#34;issuer\u0026#34;:\u0026#34;https://token.actions.githubusercontent.com\u0026#34;,\u0026#34;subject\u0026#34;:\u0026#34;repo:\u0026#39;$YOUR_GITHUB_ORG\u0026#39;/\u0026#39;$YOUR_REPO_NAME\u0026#39;:pull_request\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;Trust pull requests\u0026#34;,\u0026#34;audiences\u0026#34;:[\u0026#34;api://AzureADTokenExchange\u0026#34;]}\u0026#39; echo \u0026#34;ARM_CLIENT_ID: $AZURE_CLIENT_ID\u0026#34; echo \u0026#34;ARM_TENANT_ID: $(az account show --query tenantId -o tsv)\u0026#34; echo \u0026#34;ARM_SUBSCRIPTION_ID: $AZURE_SUBSCRIPTION_ID\u0026#34; echo \u0026#34;RESOURCE_GROUP_NAME: $RESOURCE_GROUP_NAME\u0026#34; echo \u0026#34;STORAGE_ACCOUNT_NAME: $STORAGE_ACCOUNT_NAME\u0026#34; echo \u0026#34;CONTAINER_NAME: $CONTAINER_NAME\u0026#34; echo \u0026#34;KEY: terraform.tfstate\u0026#34; Save the echo\u0026rsquo;d values after the script is finished! These will be used in the next step to configure the Terraform backend. You should now have a service principal with these two tokens. One for authenticating in a pull request to main and another for pushing to main. Terraform Configuration For the azurerm provider to work with OIDC, we need to change some settings and add some variables.\nIn your repository, create a Terraform configuration file, for example versions.tf. Open the newly created file and paste in the below code block. Notice the provider settings, use_oidc and use_azuread_auth. These must be set to true for our pipeline to authenticate. Input the missing variables in backend \u0026quot;azurerm\u0026quot; using the values from the previous script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 terraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt;4.0\u0026#34; } } backend \u0026#34;azurerm\u0026#34; { use_oidc = true use_azuread_auth = true resource_group_name = \u0026#34;\u0026#34; storage_account_name = \u0026#34;\u0026#34; container_name = \u0026#34;\u0026#34; key = \u0026#34;\u0026#34; } } provider \u0026#34;azurerm\u0026#34; { features {} } GitHub Actions Workflow In this next step, we are going to create the workflow used by GitHub Actions to deploy our Terraform resources.\nIn your repository, create a new directory and file called .github/workflows/terraform.yml. This file will contain our pipeline. Open the newly created file and paste in the below code block. This workflow is built with some best practices in mind. The plan job runs on Pull Requests to main, uploads the plan binary as an artifact, and creates a comment in your PR with the plan changes. The apply job runs on Push to main, downloads the plan binary from the artifact, and pushes your changes. For the pipeline to work properly, edit the env values using the output from the previous script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 name: \u0026#39;Terraform OIDC\u0026#39; on: # Trigger on pull requests targeting the main branch pull_request: types: [\u0026#34;opened\u0026#34;, \u0026#34;synchronize\u0026#34;, \u0026#34;reopened\u0026#34;] branches: [\u0026#34;main\u0026#34;] paths: [\u0026#34;terraform_oidc/**.tf\u0026#34;] # Trigger on pushes (merges) to the main branch push: branches: [\u0026#34;main\u0026#34;] paths: [\u0026#34;terraform_oidc/**.tf\u0026#34;] env: ARM_CLIENT_ID: \u0026#34;\u0026#34; ARM_SUBSCRIPTION_ID: \u0026#34;\u0026#34; ARM_TENANT_ID: \u0026#34;\u0026#34; # Required for OIDC login and for posting PR comments permissions: id-token: write contents: read pull-requests: write jobs: plan: name: \u0026#39;Terraform Plan\u0026#39; runs-on: ubuntu-latest if: \u0026gt;- (github.event_name == \u0026#39;pull_request\u0026#39;) || (github.event_name == \u0026#39;push\u0026#39; \u0026amp;\u0026amp; github.ref == \u0026#39;refs/heads/main\u0026#39;) steps: - name: Checkout uses: actions/checkout@v5 - name: Azure Login uses: azure/login@v2 with: client-id: ${{ env.ARM_CLIENT_ID }} tenant-id: ${{ env.ARM_TENANT_ID }} subscription-id: ${{ env.ARM_SUBSCRIPTION_ID }} - name: Setup Terraform uses: hashicorp/setup-terraform@v3 - name: Terraform Init run: terraform init - name: Terraform Validate run: terraform validate -no-color - name: Terraform Plan id: plan run: terraform plan -no-color -out=\u0026#34;tfplan\u0026#34; continue-on-error: true - name: Post Plan Comment to PR if: github.event_name == \u0026#39;pull_request\u0026#39; uses: actions/github-script@v8 env: PLAN: \u0026#34;terraform\\n${{ steps.plan.outputs.stdout }}\u0026#34; with: script: | const { PLAN } = process.env; const output = `#### Terraform Plan 📖\\`${{ github.event.pull_request.head.sha }}\\` \u0026lt;details\u0026gt;\u0026lt;summary\u0026gt;Show Plan\u0026lt;/summary\u0026gt; \\`\\`\\`\\n${PLAN}\\n\\`\\`\\` \u0026lt;/details\u0026gt; *Pushed by: @${{ github.actor }}, Action: \\`${{ github.event_name }}\\`*`; await github.rest.issues.createComment({ owner: context.repo.owner, repo: context.repo.repo, issue_number: context.issue.number, body: output }); if (\u0026#34;${{ steps.plan.outcome }}\u0026#34; == \u0026#34;failure\u0026#34;) { process.exit(1); } - name: Upload Terraform Plan Artifact if: steps.plan.outcome == \u0026#39;success\u0026#39; uses: actions/upload-artifact@v4 with: name: terraform-plan path: tfplan retention-days: 1 apply: name: \u0026#39;Terraform Apply\u0026#39; needs: plan runs-on: ubuntu-latest if: github.event_name == \u0026#39;push\u0026#39; \u0026amp;\u0026amp; github.ref == \u0026#39;refs/heads/main\u0026#39; \u0026amp;\u0026amp; needs.Plan.result == \u0026#39;success\u0026#39; steps: - name: Checkout uses: actions/checkout@v5 - name: Azure Login uses: azure/login@v2 with: client-id: ${{ env.ARM_CLIENT_ID }} tenant-id: ${{ env.ARM_TENANT_ID }} subscription-id: ${{ env.ARM_SUBSCRIPTION_ID }} - name: Setup Terraform uses: hashicorp/setup-terraform@v3 - name: Terraform Init run: terraform init - name: Download Terraform Plan Artifact uses: actions/download-artifact@v5 with: name: terraform-plan - name: Terraform Apply run: terraform apply -auto-approve \u0026#34;tfplan\u0026#34; If you\u0026rsquo;ve followed all the steps, run the pipeline, it should then authenticate to Azure using OIDC, create a plan on PRs, and deploy changes on merges to main.\nResources \u0026amp; Links For more detailed information on authentication to the Terraform backend in Azure, please refer to the official documentation.\nHashiCorp - azurerm Provider Documentation ","date":"2025-10-08T00:00:00Z","permalink":"http://localhost:1313/post/azure-terraform-github-actions-oidc/","title":"Eliminate Secrets From Your Terraform Azure Backend"},{"content":"Prerequisites Before you begin, ensure you have the following set up.\nAn Azure account with an active subscription. Make sure it has the Entra ID role Application Administrator, and Azure Owner role on the subscription. A GitHub account. Locally installed tools: Git, Azure CLI, and a code editor. (Optional) A custom domain name purchased from any registrar. Repository Setup To streamline the process, a GitHub template is provided. This template contains all the necessary Terraform, GitHub Actions, and Hugo boilerplate files.\nNavigate to the GitHub template repository. Click Use this template and select Create a new repository. Provide a name and optional description, then click Create repository. Clone your new repository to your local machine. Terraform Backend Before Terraform can manage your Azure resources, we must create a secure, remote backend. This involves running a script to provision an Azure Storage Account for Terraform\u0026rsquo;s state and a Service Principal for authentication.\nLocate the file ./tools/azure-backend.sh in your repository. Open the file and configure the environment variables with your specific details. 1 2 3 4 5 6 7 RESOURCE_GROUP_NAME=\u0026#34;hugo\u0026#34; STORAGE_ACCOUNT_NAME=\u0026#34;tfstatehugo\u0026lt;INSERT RANDOM NUMBERS\u0026gt;\u0026#34; # must be globally unique\u0026#34; CONTAINER_NAME=\u0026#34;tfstate\u0026#34; LOCATION=\u0026#34;\u0026#34; # e.g. \u0026#34;eastus\u0026#34; or \u0026#34;norwayeast\u0026#34; APP_NAME=\u0026#34;Hugo - GitHub Actions\u0026#34; YOUR_GITHUB_ORG=\u0026#34;\u0026#34; # e.g. \u0026#34;my-github-username\u0026#34; or \u0026#34;my-github-organization\u0026#34; YOUR_REPO_NAME=\u0026#34;\u0026#34; Run the script from your terminal. You\u0026rsquo;ll be prompted to log into Azure. After completion, the script will output several important values. Save these, as you will need them for the next steps. GitHub Authentication To allow secure communication between Azure and GitHub, you need to create two sets of credentials: a Personal Access Token (PAT) for Azure to update your repository, and GitHub secrets for your workflows to authenticate with Azure.\nCreate a Personal Access Token (PAT) Create a fine-grained PAT on GitHub. Grant it the following repository permissions: Contents: Read and write Workflows: Read and write Secrets: Read and write Copy the generated token and save it securely. Create GitHub Secrets In your repository settings, go to Secrets and variables \u0026gt; Actions. Create the following secrets using the values from the backend script output and the PAT you just created: AZURE_CLIENT_ID AZURE_SUBSCRIPTION_ID AZURE_TENANT_ID GH_PAT Deploy Infrastructure With authentication configured, you can now deploy the Azure infrastructure using Terraform. This is handled automatically through a pull request workflow.\nCreate a new Git branch. Open ./infra/variables.tf and edit the default values. For now, leave the custom domain variables unchanged. Commit your changes and push the branch to GitHub. Create a pull request. The workflow will automatically run and show you a terraform plan in a PR comment. Review the plan. If it\u0026rsquo;s correct, merge the pull request to deploy the resources. After merging, a new workflow file and a new secret will be automatically added to your repository by Azure. Deploy Hugo Website The final step is to configure the deployment pipeline to correctly build your Hugo site from the src directory and deploy it to the new Static Web App.\nCreate a new branch, ensuring you\u0026rsquo;ve pulled the latest changes from main. Find the new workflow file at ./.github/workflows/azure-static-web-apps-\u0026lt;RANDOM_ID\u0026gt;.yml. Modify the file to set the app_location and output_location. 1 2 3 4 5 6 ... action: \u0026#34;upload\u0026#34; app_location: \u0026#34;src\u0026#34; # \u0026lt;-- api_location: \u0026#34;\u0026#34; output_location: \u0026#34;public\u0026#34; # \u0026lt;-- ... Commit the change, create a pull request, and merge it. Once the pipeline succeeds, your Hugo website will be live! Check the Azure Portal for the URL. Hugo Configuration Once your site is deployed, you can manage its content and configuration. Here’s how to work with your Hugo site\u0026rsquo;s structure.\nThe hugo.toml file is the main configuration for your site. Here you can add your title, description, and generally change a bunch of settings. NB! For your domain to work you\u0026rsquo;ll have to customize the baseURL to match your domain. ./content/page/*contains menu pages, like the About page. ./content/post/* contains your blog posts. Add a new directory for each post with an index.md file. At the top of your index.md file, add YAML frontmatter to configure the post. Here\u0026rsquo;s an example with most of the possible config: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 --- title: Example slug: example # This is the name that will be in your URL, e.g. https://domain.tld/post/example description: Hugo, the world\u0026#39;s fastest framework for building websites date: \u0026#39;2019-02-28\u0026#39; lastmod: \u0026#39;2020-10-09\u0026#39; categories: - Example tags: - Example aliases: - example license: CC BY-NC-ND image: cover.jpg # Resolution: 1000 px × 667 px weight: 1 # You can add weight to some posts to override the default sorting (date descending) menu: main: weight: 1 params: icon: user --- Below the frontmatter, write your content in Markdown. Push changes to the main branch to trigger an automatic deployment. (Optional) Add a Custom Domain If you wish to use your own domain, you\u0026rsquo;ll need to update the Terraform configuration and then validate the domain in the Azure portal.\nIn ./infra/variables.tf, set add_custom_domain to true and custom_domain to your domain name. Commit, push, and merge these changes via a pull request. In the Azure Portal, find your Static Web App and go to the Custom domains section. Follow the instructions to add a TXT record (this is already created) and an A record to your domain\u0026rsquo;s DNS settings at your registrar. This is how the DNS configuration should look like at your registrar. This example uses Cloudflare: Wait for DNS propagation. This may take a few minutes to a few hours. Resources \u0026amp; Links For more detailed information on Hugo and the theme used in the template, please refer to the official documentation.\nHugo Docs Stack Theme Docs Stack Theme Starter Repository ","date":"2025-09-21T00:00:00Z","permalink":"http://localhost:1313/post/hugo-site-azure/","title":"Build a Hugo Website on Azure"},{"content":"Introduction Generative AI can act as a powerful study partner, helping you refine your knowledge and test your skills efficiently. Here\u0026rsquo;s a simple, effective workflow to integrate AI into your study routine.\nPrerequisites Access to Google Gemini 2.5 Pro. Build Your Foundation Before using AI, you need raw material. Start by enrolling in a course and start taking your own notes. Focus on capturing key concepts, definitions, and processes related to the exam objectives. Don\u0026rsquo;t worry about perfection, the goal is to create a solid information base.\nEnhance Your Notes Once you have your notes, use AI to enhance them. This step helps clarify complex topics, correct inaccuracies, and structure the content for better retention.\nLogin to gemini.google.com and open a new chat. Use the below prompt template and replace the context variables with your own. Remember to include your own notes at the bottom. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Role: IT Certification Expert Task: Analyze, refine, and enhance my personal study notes for the specified IT certification exam. Context: - Exam: Microsoft Certified - Azure Developer Associate - Assessed skills/domains: - Develop Azure compute solutions - Develop for Azure storage - Implement Azure security - Monitor, troubleshoot, and optimize Azure solutions - Connect to and consume Azure services and third-party services Instructions: 1. Correct and clarify: Identify and correct any technical inaccuracies in my notes. Rephrase complex topics for better clarity. 2. Structure: Reorganize the content into a logical structure using markdown (headings, bullet points, tables, and bold text) for improved readability. 3. Enhance: Expand on the key concepts by adding critical details, simple analogies, or examples that are relevant to the exam objectives. 4. Focus: Ensure the final output is a concise, accurate, and exam-focused study guide based on my initial input. My Notes: [Paste your raw study notes here] Create a Practice Exam Passive reading is boring. Use AI to generate practice exams that simulate the real exam, helping you identify weak spots and get comfortable with the question format.\nNote: Gemini 2.5 Pro is the only one I know that supports this feature.\nLogin to gemini.google.com and open a new chat. Tap the Tools button, then choose Guided Learning. Use the below prompt template and replace the context variables with your own. NB! You can optionally add your personal notes. 1 2 3 4 5 6 7 8 9 10 11 12 13 Role: IT Certification Exam Simulator Task: Generate an interactive multiple-choice exam natively. Context: - Exam: Microsoft Certified - Azure Developer Associate - Assessed skills/domains: - Develop Azure compute solutions - Develop for Azure storage - Implement Azure security - Monitor, troubleshoot, and optimize Azure solutions - Connect to and consume Azure services and third-party services - Number of questions: 20 This is how questions look. You can optionally view a hint for the question if you\u0026rsquo;re struggling, and it will show you if you chose the correct one or not. When you\u0026rsquo;re finished you\u0026rsquo;ll see a summary page like this. You can tell it to analyze the results so you get to know what you should study more, you can ask it to create flash cards based on the same test, and you can tell it to create a study guide. You can even share it! Just press the share button and send the link. Schedule and Pass the Exam With your AI-enhanced notes and targeted practice, you\u0026rsquo;ll be well-prepared. Review your materials one last time and schedule your exam!\n","date":"2025-09-21T00:00:00Z","permalink":"http://localhost:1313/post/genai-certification-study/","title":"Study With Generative AI"},{"content":"Introduction Yep. I hate naming things.\nMost, if not all companies, loves to create global naming conventions for their resources. These conventions often become counterproductive and make creating resources a chore. Also, when you\u0026rsquo;re working with multiple customers at once, this becomes an even greater challenge.\nThe Hidden Costs of Complex Names Cognitive overhead A name like app-p-nwe-rg isn\u0026rsquo;t easy to read; it\u0026rsquo;s a code that needs deciphering. This adds mental friction for everyone who interacts with the system. Brittleness and lies Cloud resources are dynamic. What happens when the app in nwe (Norway East) is migrated to West Europe? Renaming is often impossible, so the name becomes a lie, making it even harder to understand. Maintenance and enforcement You have to write extensive documentation for the naming convention, build complex validation rules (e.g., regex in pipelines), and constantly update both. This is a time sink that doesn\u0026rsquo;t deliver value. Different world views People are different. Some will try to change it to support their specific view of the world. This adds friction. Names for Humans, Tags for Machines Resource names should describe its purpose, while its metadata should be handled by tags.\nMeaningful names A developer should be able to name a resource without consulting a document. For globally unique resources, appending a random string or integer is an effective solution. Structured metadata with tags Tags are the native cloud solution for metadata. They are key-value pairs that are searchable, can be used for cost allocation, and can trigger automated policies. Example Instead of app-p-nwe-rg, the resource would simply be named my-application. The metadata is captured in tags: Key Value environment production cost_center 12345 owner team-alpha application my-application The resource\u0026rsquo;s type (Resource Group) and location (Norway East) are already first-class properties of the resource, so embedding them in the name is redundant.\nHow to Make it Work Automate enforcement: use cloud-native tools like Azure Policy or AWS Service Control Policies (SCPs) to enforce the presence of required tags. You can create policies that deny the creation of any resource missing a tag. This is far more effective than manual enforcement.\nKeep it simple, start with a minimal set of required tags and only add more as a clear need arises.\nConclusion By letting names be simple identifiers and using tags for metadata, teams can move faster, reduce maintenance overhead, and build a more flexible and understandable environment.\nLet names be names, and let metadata be tags\n","date":"2025-09-21T00:00:00Z","permalink":"http://localhost:1313/post/naming-conventions/","title":"Why Your Naming Convention Is Slowing You Down"}]